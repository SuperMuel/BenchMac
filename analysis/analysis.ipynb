{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BenchMAC Analysis V2\n",
    "\n",
    "This notebook provides a comprehensive analysis of all BenchMAC experiments, including both successful and failed runs.\n",
    "\n",
    "Unlike the first analysis notebook which only considered completed experiments, this version:\n",
    "- Analyzes failed experiments and categorizes failure reasons\n",
    "- Handles experiments with empty diffs (now treated as completed with zero metrics)\n",
    "- Identifies missing experiments and evaluations\n",
    "- Provides actionable recommendations for re-running experiments with adjusted limits\n",
    "\n",
    "## Approach\n",
    "\n",
    "1. Load all experiments and evaluations from disk\n",
    "2. Compute experiment tasks as the Cartesian product: instances × agent_configs\n",
    "3. Match each task to experiments and evaluations\n",
    "4. Identify and categorize failures (steps limit, cost limit, other errors)\n",
    "5. Filter out agent configs with excessive cost-related failures\n",
    "6. Create final dataset: one experiment + evaluation per task\n",
    "7. Perform analysis and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from pydantic import TypeAdapter\n",
    "\n",
    "from bench_mac.core.models import (\n",
    "    EvaluationCompleted,\n",
    "    EvaluationFailed,\n",
    "    EvaluationResult,\n",
    ")\n",
    "from experiments.models import (\n",
    "    AgentConfig,\n",
    "    CompletedExperiment,\n",
    "    ExperimentResult,\n",
    "    FailedExperiment,\n",
    "    MiniSweAgentConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load All Experiments and Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BENCHMAC_DIR = Path(\"../.benchmac\")\n",
    "EXPERIMENTS_DIR = BENCHMAC_DIR / \"experiments\" / \"results\"\n",
    "EVALUATIONS_DIR = BENCHMAC_DIR / \"evaluations\"\n",
    "assert EXPERIMENTS_DIR.exists()\n",
    "assert EVALUATIONS_DIR.exists()\n",
    "\n",
    "# Instances to exclude from analysis\n",
    "EXCLUDED_INSTANCES = {\n",
    "    \"akveo__ngx-admin_v15_to_v16\",  # Known problematic instance\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiments(\n",
    "    experiments_dir: Path,\n",
    ") -> tuple[list[CompletedExperiment], list[FailedExperiment]]:\n",
    "    \"\"\"Load all experiments from JSON files.\"\"\"\n",
    "    completed: list[CompletedExperiment] = []\n",
    "    failed: list[FailedExperiment] = []\n",
    "\n",
    "    for json_file in experiments_dir.rglob(\"*.json\"):\n",
    "        with json_file.open(\"r\") as f:\n",
    "            experiment = ExperimentResult.model_validate_json(f.read()).root\n",
    "\n",
    "            match experiment:\n",
    "                case CompletedExperiment():\n",
    "                    completed.append(experiment)\n",
    "                case FailedExperiment():\n",
    "                    failed.append(experiment)\n",
    "\n",
    "    return completed, failed\n",
    "\n",
    "\n",
    "def load_evaluations(\n",
    "    evaluations_dir: Path,\n",
    ") -> tuple[list[EvaluationCompleted], list[EvaluationFailed]]:\n",
    "    \"\"\"Load all evaluations from JSONL files.\"\"\"\n",
    "    completed: list[EvaluationCompleted] = []\n",
    "    failed: list[EvaluationFailed] = []\n",
    "\n",
    "    eval_adapter = TypeAdapter(EvaluationResult)\n",
    "\n",
    "    for jsonl_file in evaluations_dir.rglob(\"*.jsonl\"):\n",
    "        with jsonl_file.open(\"r\") as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    eval_result = eval_adapter.validate_python(json.loads(line))\n",
    "                    match eval_result:\n",
    "                        case EvaluationCompleted():\n",
    "                            completed.append(eval_result)\n",
    "                        case EvaluationFailed():\n",
    "                            failed.append(eval_result)\n",
    "\n",
    "    return completed, failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading experiments...\n",
      "Loading evaluations...\n",
      "\n",
      "Loaded 168 completed experiments\n",
      "Loaded 35 failed experiments\n",
      "Loaded 172 completed evaluations\n",
      "Loaded 0 failed evaluations\n"
     ]
    }
   ],
   "source": [
    "# Load all data\n",
    "print(\"Loading experiments...\")\n",
    "completed_experiments, failed_experiments = load_experiments(EXPERIMENTS_DIR)\n",
    "\n",
    "print(\"Loading evaluations...\")\n",
    "completed_evaluations, failed_evaluations = load_evaluations(EVALUATIONS_DIR)\n",
    "\n",
    "print(f\"\\nLoaded {len(completed_experiments)} completed experiments\")\n",
    "print(f\"Loaded {len(failed_experiments)} failed experiments\")\n",
    "print(f\"Loaded {len(completed_evaluations)} completed evaluations\")\n",
    "print(f\"Loaded {len(failed_evaluations)} failed evaluations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial Filtering and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering excluded instances:\n",
      "  168 completed experiments\n",
      "  35 failed experiments\n"
     ]
    }
   ],
   "source": [
    "# Filter out excluded instances\n",
    "completed_experiments = [\n",
    "    e for e in completed_experiments if e.task.instance_id not in EXCLUDED_INSTANCES\n",
    "]\n",
    "failed_experiments = [\n",
    "    e for e in failed_experiments if e.task.instance_id not in EXCLUDED_INSTANCES\n",
    "]\n",
    "\n",
    "print(\"After filtering excluded instances:\")\n",
    "print(f\"  {len(completed_experiments)} completed experiments\")\n",
    "print(f\"  {len(failed_experiments)} failed experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experiments with empty diffs: 7\n"
     ]
    }
   ],
   "source": [
    "# Count experiments with empty diffs\n",
    "empty_diff_experiments = [\n",
    "    e for e in completed_experiments if e.submission.model_patch == \"\"\n",
    "]\n",
    "\n",
    "print(f\"\\nExperiments with empty diffs: {len(empty_diff_experiments)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Unique Instances and Agent Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 unique benchmark instances:\n",
      "  - gothinkster__angular-realworld-example-app_v11_to_v12\n",
      "  - gothinkster__angular-realworld-example-app_v12_to_v13\n",
      "  - gothinkster__angular-realworld-example-app_v13_to_v14\n",
      "  - gothinkster__angular-realworld-example-app_v14_to_v15\n",
      "  - gothinkster__angular-realworld-example-app_v15_to_v16\n",
      "  - gothinkster__angular-realworld-example-app_v16_to_v17\n",
      "  - gothinkster__angular-realworld-example-app_v17_to_v18\n",
      "  - gothinkster__angular-realworld-example-app_v18_to_v19\n",
      "  - gothinkster__angular-realworld-example-app_v19_to_v20\n"
     ]
    }
   ],
   "source": [
    "# Extract unique instances\n",
    "all_instance_ids = {\n",
    "    exp.task.instance_id for exp in completed_experiments + failed_experiments\n",
    "}\n",
    "\n",
    "instance_ids = sorted(all_instance_ids)\n",
    "\n",
    "print(f\"Found {len(instance_ids)} unique benchmark instances:\")\n",
    "for iid in instance_ids:\n",
    "    print(f\"  - {iid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 21 unique agent configurations:\n",
      "  - angular-schematics/789e301f\n",
      "  - swe-agent-mini/anthropic/claude-opus-4-1-20250805@modelkw-916a2d40@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "    - anthropic/claude-opus-4-1-20250805\n",
      "    - {'temperature': 0.0}\n",
      "  - swe-agent-mini/anthropic/claude-sonnet-4-20250514@modelkw-916a2d40@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "    - anthropic/claude-sonnet-4-20250514\n",
      "    - {'temperature': 0.0}\n",
      "  - swe-agent-mini/anthropic/claude-sonnet-4-5-20250929@modelkw-916a2d40@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "    - anthropic/claude-sonnet-4-5-20250929\n",
      "    - {'temperature': 0.0}\n",
      "  - swe-agent-mini/gemini/gemini-2.5-flash-lite@modelkw-916a2d40@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "    - gemini/gemini-2.5-flash-lite\n",
      "    - {'temperature': 0.0}\n",
      "  - swe-agent-mini/gemini/gemini-2.5-flash@modelkw-916a2d40@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "    - gemini/gemini-2.5-flash\n",
      "    - {'temperature': 0.0}\n",
      "  - swe-agent-mini/gemini/gemini-2.5-pro@modelkw-06f54fcf@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "    - gemini/gemini-2.5-pro\n",
      "    - {'temperature': 0.0, 'thinkingBudget': -1}\n",
      "  - swe-agent-mini/mistral/devstral-medium-2507@modelkw-2b574f61@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "    - mistral/devstral-medium-2507\n",
      "    - {'temperature': 0.15, 'top_p': 1.0, 'random_seed': 42}\n",
      "  - swe-agent-mini/mistral/devstral-small-2507@modelkw-2b574f61@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "    - mistral/devstral-small-2507\n",
      "    - {'temperature': 0.15, 'top_p': 1.0, 'random_seed': 42}\n",
      "  - swe-agent-mini/mistral/magistral-medium-2509@modelkw-b2b38518@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "    - mistral/magistral-medium-2509\n",
      "    - {'temperature': 0.7, 'top_p': 1.0, 'random_seed': 42}\n",
      "  - swe-agent-mini/mistral/magistral-small-2509@modelkw-813f7a1c@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "    - mistral/magistral-small-2509\n",
      "    - {'temperature': 0.7, 'top_p': 0.95, 'random_seed': 42}\n",
      "  - swe-agent-mini/mistral/mistral-medium-2508@modelkw-7322c1f7@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "    - mistral/mistral-medium-2508\n",
      "    - {'temperature': 0.3, 'top_p': 1.0, 'random_seed': 42}\n",
      "  - swe-agent-mini/mistral/mistral-small-2506@modelkw-7322c1f7@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "    - mistral/mistral-small-2506\n",
      "    - {'temperature': 0.3, 'top_p': 1.0, 'random_seed': 42}\n",
      "  - swe-agent-mini/openai/gpt-5-2025-08-07@modelkw-4625f083@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "    - openai/gpt-5-2025-08-07\n",
      "    - {'verbosity': 'medium', 'reasoning_effort': 'medium', 'seed': 42}\n",
      "  - swe-agent-mini/openai/gpt-5-codex@modelkw-4625f083@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "    - openai/gpt-5-codex\n",
      "    - {'verbosity': 'medium', 'reasoning_effort': 'medium', 'seed': 42}\n",
      "  - swe-agent-mini/openai/gpt-5-mini-2025-08-07@modelkw-4625f083@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "    - openai/gpt-5-mini-2025-08-07\n",
      "    - {'verbosity': 'medium', 'reasoning_effort': 'medium', 'seed': 42}\n",
      "  - swe-agent-mini/openai/gpt-5-nano-2025-08-07@modelkw-4625f083@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "    - openai/gpt-5-nano-2025-08-07\n",
      "    - {'verbosity': 'medium', 'reasoning_effort': 'medium', 'seed': 42}\n",
      "  - swe-agent-mini/xai/grok-4-0709@modelkw-916a2d40@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "    - xai/grok-4-0709\n",
      "    - {'temperature': 0.0}\n",
      "  - swe-agent-mini/xai/grok-4-fast-non-reasoning@modelkw-916a2d40@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "    - xai/grok-4-fast-non-reasoning\n",
      "    - {'temperature': 0.0}\n",
      "  - swe-agent-mini/xai/grok-4-fast-reasoning@modelkw-916a2d40@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "    - xai/grok-4-fast-reasoning\n",
      "    - {'temperature': 0.0}\n",
      "  - swe-agent-mini/xai/grok-code-fast-1-0825@modelkw-916a2d40@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "    - xai/grok-code-fast-1-0825\n",
      "    - {'temperature': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Extract unique agent configs\n",
    "type AgentConfigKey = str\n",
    "agent_configs_dict: dict[AgentConfigKey, AgentConfig] = {}\n",
    "\n",
    "for exp in completed_experiments + failed_experiments:\n",
    "    config = exp.task.agent_config\n",
    "    key = config.key\n",
    "    if key not in agent_configs_dict:\n",
    "        agent_configs_dict[key] = config\n",
    "\n",
    "agent_configs = sorted(agent_configs_dict.values(), key=lambda x: x.key)\n",
    "\n",
    "print(f\"\\nFound {len(agent_configs)} unique agent configurations:\")\n",
    "for ac in agent_configs:\n",
    "    print(f\"  - {ac.key}\")\n",
    "    if isinstance(ac, MiniSweAgentConfig):\n",
    "        print(f\"    - {ac.model_name}\")\n",
    "        model_kwargs = ac.model_kwargs\n",
    "        if model_kwargs:\n",
    "            print(f\"    - {model_kwargs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute Experiment Tasks and Match to Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total experiment tasks: 189\n",
      "  = 9 instances × 21 agent configs\n"
     ]
    }
   ],
   "source": [
    "# Compute all possible experiment tasks (Cartesian product)\n",
    "from experiments.models import ExperimentTask\n",
    "\n",
    "experiment_tasks = [\n",
    "    ExperimentTask(instance_id=instance_id, agent_config=agent_config)\n",
    "    for instance_id in instance_ids\n",
    "    for agent_config in agent_configs\n",
    "]\n",
    "\n",
    "print(f\"Total experiment tasks: {len(experiment_tasks)}\")\n",
    "print(f\"  = {len(instance_ids)} instances × {len(agent_configs)} agent configs\")  # noqa: RUF001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group experiments by task\n",
    "experiments_by_task: dict[\n",
    "    ExperimentTask, list[FailedExperiment | CompletedExperiment]\n",
    "] = defaultdict(list)\n",
    "\n",
    "for exp in completed_experiments:\n",
    "    task = ExperimentTask(\n",
    "        instance_id=exp.task.instance_id, agent_config=exp.task.agent_config\n",
    "    )\n",
    "    experiments_by_task[task].append(exp)\n",
    "\n",
    "for exp in failed_experiments:\n",
    "    task = ExperimentTask(\n",
    "        instance_id=exp.task.instance_id, agent_config=exp.task.agent_config\n",
    "    )\n",
    "    experiments_by_task[task].append(exp)\n",
    "\n",
    "assert sum(len(exps) for exps in experiments_by_task.values()) == len(\n",
    "    completed_experiments\n",
    ") + len(failed_experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tasks with 0 experiments: 0\n",
      "\n",
      "Tasks with >1 experiments: 12\n",
      "  (This is not a problem - we keep the latest completed experiment)\n",
      "    - gothinkster__angular-realworld-example-app_v11_to_v12 with swe-agent-mini/anthropic/claude-opus-4-1-20250805@modelkw-916a2d40@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc: 2 experiments\n",
      "      - 2 failed, 0 completed\n",
      "    - gothinkster__angular-realworld-example-app_v11_to_v12 with swe-agent-mini/gemini/gemini-2.5-flash-lite@modelkw-916a2d40@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc: 2 experiments\n",
      "      - 1 failed, 1 completed\n",
      "    - gothinkster__angular-realworld-example-app_v11_to_v12 with swe-agent-mini/mistral/devstral-small-2507@modelkw-2b574f61@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc: 2 experiments\n",
      "      - 2 failed, 0 completed\n",
      "    - gothinkster__angular-realworld-example-app_v11_to_v12 with swe-agent-mini/mistral/magistral-small-2509@modelkw-813f7a1c@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc: 2 experiments\n",
      "      - 0 failed, 2 completed\n",
      "    - gothinkster__angular-realworld-example-app_v11_to_v12 with swe-agent-mini/mistral/mistral-small-2506@modelkw-7322c1f7@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc: 2 experiments\n",
      "      - 0 failed, 2 completed\n",
      "    - gothinkster__angular-realworld-example-app_v12_to_v13 with swe-agent-mini/anthropic/claude-opus-4-1-20250805@modelkw-916a2d40@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc: 3 experiments\n",
      "      - 3 failed, 0 completed\n",
      "    - gothinkster__angular-realworld-example-app_v12_to_v13 with swe-agent-mini/mistral/devstral-small-2507@modelkw-2b574f61@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc: 3 experiments\n",
      "      - 3 failed, 0 completed\n",
      "    - gothinkster__angular-realworld-example-app_v13_to_v14 with swe-agent-mini/gemini/gemini-2.5-flash-lite@modelkw-916a2d40@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc: 2 experiments\n",
      "      - 1 failed, 1 completed\n",
      "    - gothinkster__angular-realworld-example-app_v13_to_v14 with swe-agent-mini/mistral/mistral-small-2506@modelkw-7322c1f7@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc: 2 experiments\n",
      "      - 2 failed, 0 completed\n",
      "    - gothinkster__angular-realworld-example-app_v13_to_v14 with swe-agent-mini/openai/gpt-5-codex@modelkw-4625f083@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc: 2 experiments\n",
      "      - 1 failed, 1 completed\n",
      "    - gothinkster__angular-realworld-example-app_v13_to_v14 with swe-agent-mini/xai/grok-4-0709@modelkw-916a2d40@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc: 2 experiments\n",
      "      - 2 failed, 0 completed\n",
      "    - gothinkster__angular-realworld-example-app_v15_to_v16 with swe-agent-mini/mistral/magistral-medium-2509@modelkw-b2b38518@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc: 2 experiments\n",
      "      - 1 failed, 1 completed\n"
     ]
    }
   ],
   "source": [
    "# Analyze task coverage\n",
    "tasks_with_no_experiments = [\n",
    "    task\n",
    "    for task in experiment_tasks\n",
    "    if task not in experiments_by_task or not experiments_by_task[task]\n",
    "]\n",
    "\n",
    "tasks_with_multiple_experiments = [\n",
    "    (task, len(experiments_by_task[task]))\n",
    "    for task in experiment_tasks\n",
    "    if len(experiments_by_task[task]) > 1\n",
    "]\n",
    "\n",
    "print(f\"\\nTasks with 0 experiments: {len(tasks_with_no_experiments)}\")\n",
    "if tasks_with_no_experiments:\n",
    "    print(\"  Consider running these experiments:\")\n",
    "    for task in tasks_with_no_experiments[:5]:  # Show first 5\n",
    "        print(f\"    - {task.instance_id} with {task.agent_config.key}\")\n",
    "    if len(tasks_with_no_experiments) > 5:\n",
    "        print(f\"    ... and {len(tasks_with_no_experiments) - 5} more\")\n",
    "\n",
    "print(f\"\\nTasks with >1 experiments: {len(tasks_with_multiple_experiments)}\")\n",
    "if tasks_with_multiple_experiments:\n",
    "    print(\"  (This is not a problem - we keep the latest completed experiment)\")\n",
    "    for task, count in tasks_with_multiple_experiments:\n",
    "        print(\n",
    "            f\"    - {task.instance_id} with {task.agent_config.key}: {count} experiments\"\n",
    "        )\n",
    "        failed, completed = (\n",
    "            [e for e in experiments_by_task[task] if isinstance(e, FailedExperiment)],\n",
    "            [\n",
    "                e\n",
    "                for e in experiments_by_task[task]\n",
    "                if isinstance(e, CompletedExperiment)\n",
    "            ],\n",
    "        )\n",
    "        print(f\"      - {len(failed)} failed, {len(completed)} completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Failed Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks with only failed experiments: 23\n"
     ]
    }
   ],
   "source": [
    "from typing import cast\n",
    "\n",
    "# Find tasks with only failed experiments\n",
    "tasks_with_only_failures: list[tuple[ExperimentTask, list[FailedExperiment]]] = []\n",
    "\n",
    "for task in experiment_tasks:\n",
    "    if task not in experiments_by_task:\n",
    "        continue\n",
    "\n",
    "    exps = experiments_by_task[task]\n",
    "    if not exps:\n",
    "        print(f\"Task {(task.instance_id, task.agent_config.key)} has no experiments\")\n",
    "        continue\n",
    "\n",
    "    if all(isinstance(e, FailedExperiment) for e in exps):\n",
    "        tasks_with_only_failures.append((task, cast(list[FailedExperiment], exps)))\n",
    "\n",
    "print(f\"Tasks with only failed experiments: {len(tasks_with_only_failures)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Failure categories:\n",
      "  steps_limit_exceeded: 14\n",
      "  cost_limit_exceeded: 9\n"
     ]
    }
   ],
   "source": [
    "STEP_LIMIT_MULTIPLE = 100  # ⚠️⚠️⚠️⚠️ Assuming step limit is a multiple of that ⚠️⚠️⚠️⚠️⚠️\n",
    "\n",
    "\n",
    "# Categorize failure reasons\n",
    "def categorize_failure(exp: FailedExperiment) -> str:\n",
    "    \"\"\"Categorize the reason for experiment failure.\"\"\"\n",
    "\n",
    "    # Check if steps limit was exceeded\n",
    "    if (\n",
    "        \"LimitsExceeded\" in exp.error\n",
    "        and exp.artifacts\n",
    "        and exp.artifacts.execution_trace\n",
    "    ):\n",
    "        num_steps = len(exp.artifacts.execution_trace.steps)\n",
    "        if num_steps % STEP_LIMIT_MULTIPLE == 0:\n",
    "            return \"steps_limit_exceeded\"\n",
    "\n",
    "    # Check error message for cost-related failures\n",
    "    if \"LimitsExceeded\" in exp.error:\n",
    "        return (\n",
    "            \"cost_limit_exceeded\"  # Assume if it's not a step limit, it's a cost limit\n",
    "        )\n",
    "\n",
    "    return \"other_error\"\n",
    "\n",
    "\n",
    "failure_categories = Counter()\n",
    "failures_by_category: dict[str, list[tuple[ExperimentTask, FailedExperiment]]] = (\n",
    "    defaultdict(list)\n",
    ")  # category -> list[(task, FailedExperiment)]\n",
    "\n",
    "for task, exps in tasks_with_only_failures:\n",
    "    latest_failure = max(exps, key=lambda e: e.ended_at)\n",
    "    category = categorize_failure(latest_failure)\n",
    "    failure_categories[category] += 1\n",
    "    failures_by_category[category].append((task, latest_failure))\n",
    "\n",
    "print(\"\\nFailure categories:\")\n",
    "for category, count in failure_categories.most_common():\n",
    "    print(f\"  {category}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Handle Cost Limit Failures\n",
    "\n",
    "Rule:\n",
    "- If more than X percent of an agent config's runs exceeded the cost limit, discard all runs from that agent\n",
    "- For remaining cost-exceeded runs, provide recommendations to re-run with higher limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent configs to discard (>33% cost failures): 2\n",
      "  swe-agent-mini/xai/grok-4-0709@modelkw-916a2d40@minisweagent...: 4/9=44% failures\n",
      "  swe-agent-mini/anthropic/claude-opus-4-1-20250805@modelkw-91...: 5/9=56% failures\n",
      "\n",
      "Agent configs needing re-runs (some cost failures): 0\n"
     ]
    }
   ],
   "source": [
    "# Set the threshold for discarding agents due to cost failures\n",
    "COST_FAILURE_DISCARD_THRESHOLD_PERCENT = 33\n",
    "\n",
    "# Count cost failures per agent config\n",
    "cost_failures_by_agent = defaultdict(int)\n",
    "total_runs_by_agent = defaultdict(int)\n",
    "\n",
    "for task in experiment_tasks:\n",
    "    agent_key = task.agent_config.key\n",
    "    total_runs_by_agent[agent_key] += 1\n",
    "\n",
    "    if task in experiments_by_task:\n",
    "        exps = experiments_by_task[task]\n",
    "        latest = max(\n",
    "            exps, key=lambda e: e.ended_at if hasattr(e, \"ended_at\") else e.started_at\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            isinstance(latest, FailedExperiment)\n",
    "            and categorize_failure(latest) == \"cost_limit_exceeded\"\n",
    "        ):\n",
    "            cost_failures_by_agent[agent_key] += 1\n",
    "\n",
    "# Identify agents to discard (>COST_FAILURE_DISCARD_THRESHOLD_PERCENT% cost failures)\n",
    "agents_to_discard: set[AgentConfigKey] = set()\n",
    "agents_needing_rerun: dict[AgentConfigKey, int] = {}\n",
    "\n",
    "for agent_key in total_runs_by_agent:\n",
    "    total = total_runs_by_agent[agent_key]\n",
    "    cost_fails = cost_failures_by_agent[agent_key]\n",
    "\n",
    "    if (\n",
    "        total > 0\n",
    "        and (cost_fails / total * 100) > COST_FAILURE_DISCARD_THRESHOLD_PERCENT\n",
    "    ):\n",
    "        agents_to_discard.add(agent_key)\n",
    "    elif cost_fails > 0:\n",
    "        agents_needing_rerun[agent_key] = cost_fails\n",
    "\n",
    "print(\n",
    "    f\"Agent configs to discard (>{COST_FAILURE_DISCARD_THRESHOLD_PERCENT}% cost failures): {len(agents_to_discard)}\"\n",
    ")\n",
    "for agent_key in agents_to_discard:\n",
    "    total = total_runs_by_agent[agent_key]\n",
    "    cost_fails = cost_failures_by_agent[agent_key]\n",
    "    print(\n",
    "        f\"  {agent_key[:60]}...: {cost_fails}/{total}={cost_fails / total * 100:2.0f}% failures\"\n",
    "    )\n",
    "\n",
    "print(\n",
    "    f\"\\nAgent configs needing re-runs (some cost failures): {len(agents_needing_rerun)}\"\n",
    ")\n",
    "for agent_key, count in agents_needing_rerun.items():\n",
    "    print(f\"  {agent_key[:60]}...: {count} failures\")\n",
    "    # Show the actual tasks for this agent_key that failed due to cost\n",
    "    failed_tasks = [\n",
    "        task\n",
    "        for task in experiment_tasks\n",
    "        if task.agent_config.key == agent_key\n",
    "        and task in experiments_by_task\n",
    "        and any(\n",
    "            isinstance(exp, FailedExperiment)\n",
    "            and categorize_failure(exp) == \"cost_limit_exceeded\"\n",
    "            for exp in experiments_by_task[task]\n",
    "        )\n",
    "    ]\n",
    "    for task in failed_tasks:\n",
    "        print(f\"    - Task instance_id: {task.instance_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show specific instances that need re-running with higher cost limits\n",
    "if agents_needing_rerun:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ACTION REQUIRED: Re-run these experiments with higher cost limits\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for task, _exp in failures_by_category[\"cost_limit_exceeded\"]:\n",
    "        agent_key = task.agent_config.key\n",
    "        if agent_key not in agents_to_discard:\n",
    "            print(f\"\\nInstance: {task.instance_id}\")\n",
    "            print(f\"Agent config: {agent_key}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"After re-running, return to this notebook to continue analysis.\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Final Dataset: One Experiment Per Task\n",
    "\n",
    "For each experiment task, we select the latest completed experiment (if available).\n",
    "Tasks with only failed experiments or from discarded agents are excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid tasks after filtering: 171\n",
      "  (Removed 18 tasks from 2 discarded agents)\n",
      "Final number of agent configs: 19\n",
      "  - angular-schematics/789e301f\n",
      "  - swe-agent-mini/anthropic/claude-sonnet-4-20250514@modelkw-916a2d40@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "  - swe-agent-mini/anthropic/claude-sonnet-4-5-20250929@modelkw-916a2d40@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "  - swe-agent-mini/gemini/gemini-2.5-flash-lite@modelkw-916a2d40@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "  - swe-agent-mini/gemini/gemini-2.5-flash@modelkw-916a2d40@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "  - swe-agent-mini/gemini/gemini-2.5-pro@modelkw-06f54fcf@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "  - swe-agent-mini/mistral/devstral-medium-2507@modelkw-2b574f61@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "  - swe-agent-mini/mistral/devstral-small-2507@modelkw-2b574f61@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "  - swe-agent-mini/mistral/magistral-medium-2509@modelkw-b2b38518@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "  - swe-agent-mini/mistral/magistral-small-2509@modelkw-813f7a1c@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "  - swe-agent-mini/mistral/mistral-medium-2508@modelkw-7322c1f7@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "  - swe-agent-mini/mistral/mistral-small-2506@modelkw-7322c1f7@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "  - swe-agent-mini/openai/gpt-5-2025-08-07@modelkw-4625f083@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "  - swe-agent-mini/openai/gpt-5-codex@modelkw-4625f083@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "  - swe-agent-mini/openai/gpt-5-mini-2025-08-07@modelkw-4625f083@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "  - swe-agent-mini/openai/gpt-5-nano-2025-08-07@modelkw-4625f083@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "  - swe-agent-mini/xai/grok-4-fast-non-reasoning@modelkw-916a2d40@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "  - swe-agent-mini/xai/grok-4-fast-reasoning@modelkw-916a2d40@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n",
      "  - swe-agent-mini/xai/grok-code-fast-1-0825@modelkw-916a2d40@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc\n"
     ]
    }
   ],
   "source": [
    "# Filter out discarded agents\n",
    "valid_tasks = [\n",
    "    task for task in experiment_tasks if task.agent_config.key not in agents_to_discard\n",
    "]\n",
    "agent_configs = sorted(\n",
    "    [ac for ac in agent_configs if ac.key not in agents_to_discard], key=lambda x: x.key\n",
    ")\n",
    "assert set(agent_configs) == {task.agent_config for task in valid_tasks}\n",
    "\n",
    "print(f\"Valid tasks after filtering: {len(valid_tasks)}\")\n",
    "print(\n",
    "    f\"  (Removed {len(experiment_tasks) - len(valid_tasks)} tasks from {len(agents_to_discard)} discarded agents)\"\n",
    ")\n",
    "# Show final number of agent configs\n",
    "print(f\"Final number of agent configs: {len(agent_configs)}\")\n",
    "for ac in agent_configs:\n",
    "    print(f\"  - {ac.key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task gothinkster__angular-realworld-example-app_v11_to_v12 with swe-agent-mini/mistral/devstral-small-2507@modelkw-2b574f61@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc has no completed experiments, so we assign the latest failed experiment\n",
      "  Failed experiment: Mini SWE Agent stopped before submission: LimitsExceeded:  (num_steps: 100)\n",
      "Task gothinkster__angular-realworld-example-app_v12_to_v13 with swe-agent-mini/mistral/devstral-small-2507@modelkw-2b574f61@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc has no completed experiments, so we assign the latest failed experiment\n",
      "  Failed experiment: Mini SWE Agent stopped before submission: LimitsExceeded:  (num_steps: 100)\n",
      "Task gothinkster__angular-realworld-example-app_v13_to_v14 with swe-agent-mini/mistral/mistral-small-2506@modelkw-7322c1f7@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc has no completed experiments, so we assign the latest failed experiment\n",
      "  Failed experiment: Mini SWE Agent stopped before submission: LimitsExceeded:  (num_steps: 100)\n",
      "Task gothinkster__angular-realworld-example-app_v14_to_v15 with swe-agent-mini/mistral/devstral-medium-2507@modelkw-2b574f61@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc has no completed experiments, so we assign the latest failed experiment\n",
      "  Failed experiment: Mini SWE Agent stopped before submission: LimitsExceeded:  (num_steps: 100)\n",
      "Task gothinkster__angular-realworld-example-app_v14_to_v15 with swe-agent-mini/mistral/mistral-small-2506@modelkw-7322c1f7@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc has no completed experiments, so we assign the latest failed experiment\n",
      "  Failed experiment: Mini SWE Agent stopped before submission: LimitsExceeded:  (num_steps: 100)\n",
      "Task gothinkster__angular-realworld-example-app_v15_to_v16 with swe-agent-mini/mistral/devstral-small-2507@modelkw-2b574f61@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc has no completed experiments, so we assign the latest failed experiment\n",
      "  Failed experiment: Mini SWE Agent stopped before submission: LimitsExceeded:  (num_steps: 100)\n",
      "Task gothinkster__angular-realworld-example-app_v15_to_v16 with swe-agent-mini/mistral/mistral-small-2506@modelkw-7322c1f7@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc has no completed experiments, so we assign the latest failed experiment\n",
      "  Failed experiment: Mini SWE Agent stopped before submission: LimitsExceeded:  (num_steps: 100)\n",
      "Task gothinkster__angular-realworld-example-app_v16_to_v17 with swe-agent-mini/mistral/devstral-small-2507@modelkw-2b574f61@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc has no completed experiments, so we assign the latest failed experiment\n",
      "  Failed experiment: Mini SWE Agent stopped before submission: LimitsExceeded:  (num_steps: 100)\n",
      "Task gothinkster__angular-realworld-example-app_v17_to_v18 with swe-agent-mini/mistral/devstral-small-2507@modelkw-2b574f61@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc has no completed experiments, so we assign the latest failed experiment\n",
      "  Failed experiment: Mini SWE Agent stopped before submission: LimitsExceeded:  (num_steps: 100)\n",
      "Task gothinkster__angular-realworld-example-app_v18_to_v19 with swe-agent-mini/gemini/gemini-2.5-flash-lite@modelkw-916a2d40@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc has no completed experiments, so we assign the latest failed experiment\n",
      "  Failed experiment: Mini SWE Agent stopped before submission: LimitsExceeded:  (num_steps: 100)\n",
      "Task gothinkster__angular-realworld-example-app_v18_to_v19 with swe-agent-mini/mistral/devstral-medium-2507@modelkw-2b574f61@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc has no completed experiments, so we assign the latest failed experiment\n",
      "  Failed experiment: Mini SWE Agent stopped before submission: LimitsExceeded:  (num_steps: 100)\n",
      "Task gothinkster__angular-realworld-example-app_v18_to_v19 with swe-agent-mini/mistral/devstral-small-2507@modelkw-2b574f61@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc has no completed experiments, so we assign the latest failed experiment\n",
      "  Failed experiment: Mini SWE Agent stopped before submission: LimitsExceeded:  (num_steps: 100)\n",
      "Task gothinkster__angular-realworld-example-app_v18_to_v19 with swe-agent-mini/mistral/mistral-small-2506@modelkw-7322c1f7@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc has no completed experiments, so we assign the latest failed experiment\n",
      "  Failed experiment: Mini SWE Agent stopped before submission: LimitsExceeded:  (num_steps: 100)\n",
      "Task gothinkster__angular-realworld-example-app_v19_to_v20 with swe-agent-mini/mistral/devstral-small-2507@modelkw-2b574f61@minisweagent-1.13.0@tasktpl-73947524@agentsettings-e3def7dc has no completed experiments, so we assign the latest failed experiment\n",
      "  Failed experiment: Mini SWE Agent stopped before submission: LimitsExceeded:  (num_steps: 100)\n",
      "\n",
      "Tasks mapped to experiments: 171\n"
     ]
    }
   ],
   "source": [
    "# Map each valid task to its latest completed experiment\n",
    "task_to_experiment: dict[ExperimentTask, CompletedExperiment | FailedExperiment] = {}\n",
    "\n",
    "for task in valid_tasks:\n",
    "    task_pretty = f\"{task.instance_id} with {task.agent_config.key}\"\n",
    "    if task not in experiments_by_task:\n",
    "        print(f\"Task {task_pretty} not found in experiments_by_task\")\n",
    "        continue\n",
    "    exps = experiments_by_task[task]\n",
    "    if not exps:\n",
    "        print(f\"Task {task_pretty} has no experiments\")\n",
    "        continue\n",
    "\n",
    "    failed = sorted(\n",
    "        [e for e in exps if isinstance(e, FailedExperiment)], key=lambda e: e.ended_at\n",
    "    )\n",
    "    completed = sorted(\n",
    "        [e for e in exps if isinstance(e, CompletedExperiment)],\n",
    "        key=lambda e: e.ended_at,\n",
    "    )\n",
    "    if not completed:\n",
    "        print(\n",
    "            f\"Task {task_pretty} has no completed experiments, so we assign the latest failed experiment\"\n",
    "        )\n",
    "        assert len(failed) >= 1\n",
    "        exp = failed[-1]\n",
    "        execution_trace = (\n",
    "            exp.artifacts.execution_trace\n",
    "            if exp.artifacts and exp.artifacts.execution_trace\n",
    "            else None\n",
    "        )\n",
    "        num_steps = len(execution_trace.steps) if execution_trace else None\n",
    "        print(f\"  Failed experiment: {exp.error} (num_steps: {num_steps})\")\n",
    "    else:\n",
    "        exp = completed[-1]\n",
    "\n",
    "    # Keep latest completed experiment\n",
    "    task_to_experiment[task] = exp\n",
    "\n",
    "print(f\"\\nTasks mapped to experiments: {len(task_to_experiment)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Match Experiments to Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group evaluations by submission ID\n",
    "from bench_mac.core.models import SubmissionID\n",
    "\n",
    "evaluations_by_submission: dict[SubmissionID, list[EvaluationResult]] = defaultdict(\n",
    "    list\n",
    ")\n",
    "\n",
    "for eval_result in completed_evaluations:\n",
    "    evaluations_by_submission[eval_result.result.submission_id].append(eval_result)\n",
    "\n",
    "for eval_result in failed_evaluations:\n",
    "    evaluations_by_submission[eval_result.submission_id].append(eval_result)\n",
    "\n",
    "assert len(evaluations_by_submission) == len(completed_evaluations) + len(\n",
    "    failed_evaluations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks with completed evaluations: 157\n",
      "Tasks without any evaluation: 0\n",
      "Tasks with only failed evaluations: 0\n"
     ]
    }
   ],
   "source": [
    "# Match each experiment to evaluations\n",
    "task_to_evaluation: dict[ExperimentTask, EvaluationResult] = {}\n",
    "tasks_without_evaluation: list[ExperimentTask] = []\n",
    "tasks_with_only_failed_evals: dict[ExperimentTask, list[EvaluationFailed]] = (\n",
    "    defaultdict(list)\n",
    ")\n",
    "\n",
    "for task, exp in task_to_experiment.items():\n",
    "    match exp:\n",
    "        case CompletedExperiment():\n",
    "            submission_id = exp.submission.submission_id\n",
    "        case FailedExperiment():\n",
    "            # FailedExperiments have no submission\n",
    "            continue\n",
    "\n",
    "    if (\n",
    "        submission_id not in evaluations_by_submission\n",
    "        or not evaluations_by_submission[submission_id]\n",
    "    ):\n",
    "        tasks_without_evaluation.append(task)\n",
    "        continue\n",
    "\n",
    "    evals = evaluations_by_submission[submission_id]\n",
    "    assert len(evals) > 0\n",
    "\n",
    "    completed_evals = [e for e in evals if isinstance(e, EvaluationCompleted)]\n",
    "    failed_evals = [e for e in evals if isinstance(e, EvaluationFailed)]\n",
    "\n",
    "    if not completed_evals:\n",
    "        tasks_with_only_failed_evals[task] = failed_evals\n",
    "        continue\n",
    "\n",
    "    # Keep latest completed evaluation\n",
    "    latest_eval = max(completed_evals, key=lambda e: e.ended_at)\n",
    "    task_to_evaluation[task] = latest_eval\n",
    "\n",
    "print(f\"Tasks with completed evaluations: {len(task_to_evaluation)}\")\n",
    "print(f\"Tasks without any evaluation: {len(tasks_without_evaluation)}\")\n",
    "print(f\"Tasks with only failed evaluations: {len(tasks_with_only_failed_evals)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show failed evaluation errors\n",
    "if tasks_with_only_failed_evals:\n",
    "    print(\"\\nFailed evaluations:\")\n",
    "    for task, evals in tasks_with_only_failed_evals.items():\n",
    "        task_pretty = f\"{task.instance_id} with {task.agent_config.key}\"\n",
    "        print(f\"  {task_pretty}:\")\n",
    "        for eval in evals:\n",
    "            print(f\"    - {eval.error}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt user to run missing evaluations\n",
    "if tasks_without_evaluation or tasks_with_only_failed_evals:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ACTION REQUIRED: Run evaluations for experiments\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nRun: uv run benchmac eval\")\n",
    "    print(\n",
    "        f\"\\nMissing evaluations: {len(tasks_without_evaluation) + len(tasks_with_only_failed_evals)}\"\n",
    "    )\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluatedExperiment = tuple[CompletedExperiment, EvaluationCompleted]\n",
    "data: dict[ExperimentTask, FailedExperiment | EvaluatedExperiment] = {}\n",
    "\n",
    "for task in valid_tasks:\n",
    "    match task_to_experiment[task]:\n",
    "        case CompletedExperiment():\n",
    "            exp = task_to_experiment[task]\n",
    "            eval = task_to_evaluation[task]\n",
    "            assert isinstance(exp, CompletedExperiment)\n",
    "            assert isinstance(eval, EvaluationCompleted)\n",
    "            data[task] = (exp, eval)\n",
    "        case FailedExperiment():\n",
    "            exp = task_to_experiment[task]\n",
    "            assert isinstance(exp, FailedExperiment)\n",
    "            data[task] = exp\n",
    "\n",
    "assert not any(task in data for task in tasks_without_evaluation)\n",
    "assert not any(task in data for task in tasks_with_only_failed_evals)\n",
    "\n",
    "tasks = list(data.keys())\n",
    "experiments = [x if isinstance(x, FailedExperiment) else x[0] for x in data.values()]\n",
    "failed_experiments = [x for x in experiments if isinstance(x, FailedExperiment)]\n",
    "completed_experiments = [x for x in experiments if isinstance(x, CompletedExperiment)]\n",
    "evaluations = [x[1] for x in data.values() if isinstance(x, tuple)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Group data by agent_config.key\n",
    "grouped_by_agent: dict[str, list[FailedExperiment | EvaluatedExperiment]] = defaultdict(\n",
    "    list\n",
    ")\n",
    "for task, result in data.items():\n",
    "    agent_key = task.agent_config.key\n",
    "    grouped_by_agent[agent_key].append(result)\n",
    "\n",
    "assert all(len(results) == len(instance_ids) for results in grouped_by_agent.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have successfully matched experiment tasks to their results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "From here, we can proceed with:\n",
    "- Computing success metrics and outcome categories\n",
    "- Analyzing patch characteristics\n",
    "- Creating leaderboards and visualizations\n",
    "- Detecting execution loops in failed experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "Now that we loaded and prepared all the data, what do we want to analyze ?\n",
    "Some open questions:\n",
    "\n",
    "- What models are the best ? Worst ? \n",
    "- What models are the most token efficient\n",
    "- What models are the most step efficient\n",
    "- What models are the fastest/slowest ?\n",
    "- What models are the most expensive, what are the cheapest ? \n",
    "  Looking at the price per token of models is not enough because:\n",
    "    Cheap per-token reasoning models can be more expensive than expensive per-token non-reasoning models, since reasoning models can generate way more tokens than non-reasoning models.\n",
    "    Some models might take more steps than others for the same task. \n",
    "    In essence, because we can't predict the amount of tokens generated by the models, we need a more reliable metric for cost. One metric could be the average cost per instance. \n",
    "    But it wouldn't give us an idea of the cost/performance ratio. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataframe construction\n",
    "\n",
    "\n",
    "evaluation metrics coming from the evaluation\n",
    "\n",
    "percentage of steps suceeded\n",
    "\n",
    "number of steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Build Analysis Dataset\n",
    "\n",
    "To enable downstream tables and leaderboards we consolidate the `data` mapping\n",
    "into a tabular structure that keeps one row per `(instance_id, agent_config)`\n",
    "pair and captures experiment/evaluation outcomes side by side.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>agent_key</th>\n",
       "      <th>agent_display_name</th>\n",
       "      <th>agent_scaffold</th>\n",
       "      <th>experiment_status</th>\n",
       "      <th>failure_category</th>\n",
       "      <th>submission_id</th>\n",
       "      <th>empty_diff</th>\n",
       "      <th>evaluation_status</th>\n",
       "      <th>evaluation_id</th>\n",
       "      <th>...</th>\n",
       "      <th>experiment_ended_at</th>\n",
       "      <th>experiment_duration_seconds</th>\n",
       "      <th>experiment_cost_usd</th>\n",
       "      <th>experiment_n_calls</th>\n",
       "      <th>experiment_step_count</th>\n",
       "      <th>evaluation_started_at</th>\n",
       "      <th>evaluation_ended_at</th>\n",
       "      <th>evaluation_duration_seconds</th>\n",
       "      <th>evaluation_step_count</th>\n",
       "      <th>evaluation_total_duration_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gothinkster__angular-realworld-example-app_v11...</td>\n",
       "      <td>angular-schematics/789e301f</td>\n",
       "      <td>angular-schematics/789e301f</td>\n",
       "      <td>angular-schematics</td>\n",
       "      <td>completed</td>\n",
       "      <td>None</td>\n",
       "      <td>0199a233-1775-71a6-8535-6ebf930e8d18</td>\n",
       "      <td>False</td>\n",
       "      <td>completed</td>\n",
       "      <td>0199a24b-d760-7d66-b2a0-bc006af02001</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-10-01 23:56:36.294169+00:00</td>\n",
       "      <td>114.877647</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gothinkster__angular-realworld-example-app_v11...</td>\n",
       "      <td>swe-agent-mini/anthropic/claude-sonnet-4-20250...</td>\n",
       "      <td>anthropic/claude-sonnet-4-20250514</td>\n",
       "      <td>swe-agent-mini</td>\n",
       "      <td>completed</td>\n",
       "      <td>None</td>\n",
       "      <td>0199a45b-a964-757e-9fc8-6524883b29da</td>\n",
       "      <td>False</td>\n",
       "      <td>completed</td>\n",
       "      <td>0199a76a-55ce-7684-9dd2-dc3e09760e18</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-10-02 10:40:13.332620+00:00</td>\n",
       "      <td>446.549921</td>\n",
       "      <td>0.920712</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gothinkster__angular-realworld-example-app_v11...</td>\n",
       "      <td>swe-agent-mini/anthropic/claude-sonnet-4-5-202...</td>\n",
       "      <td>anthropic/claude-sonnet-4-5-20250929</td>\n",
       "      <td>swe-agent-mini</td>\n",
       "      <td>completed</td>\n",
       "      <td>None</td>\n",
       "      <td>0199a45b-a965-7cad-81e6-5a240a9eb661</td>\n",
       "      <td>False</td>\n",
       "      <td>completed</td>\n",
       "      <td>0199a76e-dd8d-7726-844f-bce9d3e0f941</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-10-02 10:47:41.814046+00:00</td>\n",
       "      <td>445.896804</td>\n",
       "      <td>0.552558</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gothinkster__angular-realworld-example-app_v11...</td>\n",
       "      <td>swe-agent-mini/gemini/gemini-2.5-flash-lite@mo...</td>\n",
       "      <td>gemini/gemini-2.5-flash-lite</td>\n",
       "      <td>swe-agent-mini</td>\n",
       "      <td>completed</td>\n",
       "      <td>None</td>\n",
       "      <td>0199a90c-ed54-78d2-8916-96c6d2db864f</td>\n",
       "      <td>False</td>\n",
       "      <td>completed</td>\n",
       "      <td>0199a99e-4774-73d7-bad6-7227d16f2faf</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-10-03 08:12:29.190901+00:00</td>\n",
       "      <td>353.443725</td>\n",
       "      <td>0.020955</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gothinkster__angular-realworld-example-app_v11...</td>\n",
       "      <td>swe-agent-mini/gemini/gemini-2.5-flash@modelkw...</td>\n",
       "      <td>gemini/gemini-2.5-flash</td>\n",
       "      <td>swe-agent-mini</td>\n",
       "      <td>completed</td>\n",
       "      <td>None</td>\n",
       "      <td>0199a45b-a961-760f-8b6e-2b4230c123a4</td>\n",
       "      <td>False</td>\n",
       "      <td>completed</td>\n",
       "      <td>0199a767-95b9-730e-8e2e-6eb481e46b9f</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-10-02 10:36:10.979403+00:00</td>\n",
       "      <td>331.320460</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instance_id  \\\n",
       "0  gothinkster__angular-realworld-example-app_v11...   \n",
       "1  gothinkster__angular-realworld-example-app_v11...   \n",
       "2  gothinkster__angular-realworld-example-app_v11...   \n",
       "3  gothinkster__angular-realworld-example-app_v11...   \n",
       "4  gothinkster__angular-realworld-example-app_v11...   \n",
       "\n",
       "                                           agent_key  \\\n",
       "0                        angular-schematics/789e301f   \n",
       "1  swe-agent-mini/anthropic/claude-sonnet-4-20250...   \n",
       "2  swe-agent-mini/anthropic/claude-sonnet-4-5-202...   \n",
       "3  swe-agent-mini/gemini/gemini-2.5-flash-lite@mo...   \n",
       "4  swe-agent-mini/gemini/gemini-2.5-flash@modelkw...   \n",
       "\n",
       "                     agent_display_name      agent_scaffold experiment_status  \\\n",
       "0           angular-schematics/789e301f  angular-schematics         completed   \n",
       "1    anthropic/claude-sonnet-4-20250514      swe-agent-mini         completed   \n",
       "2  anthropic/claude-sonnet-4-5-20250929      swe-agent-mini         completed   \n",
       "3          gemini/gemini-2.5-flash-lite      swe-agent-mini         completed   \n",
       "4               gemini/gemini-2.5-flash      swe-agent-mini         completed   \n",
       "\n",
       "  failure_category                         submission_id empty_diff  \\\n",
       "0             None  0199a233-1775-71a6-8535-6ebf930e8d18      False   \n",
       "1             None  0199a45b-a964-757e-9fc8-6524883b29da      False   \n",
       "2             None  0199a45b-a965-7cad-81e6-5a240a9eb661      False   \n",
       "3             None  0199a90c-ed54-78d2-8916-96c6d2db864f      False   \n",
       "4             None  0199a45b-a961-760f-8b6e-2b4230c123a4      False   \n",
       "\n",
       "  evaluation_status                         evaluation_id  ...  \\\n",
       "0         completed  0199a24b-d760-7d66-b2a0-bc006af02001  ...   \n",
       "1         completed  0199a76a-55ce-7684-9dd2-dc3e09760e18  ...   \n",
       "2         completed  0199a76e-dd8d-7726-844f-bce9d3e0f941  ...   \n",
       "3         completed  0199a99e-4774-73d7-bad6-7227d16f2faf  ...   \n",
       "4         completed  0199a767-95b9-730e-8e2e-6eb481e46b9f  ...   \n",
       "\n",
       "               experiment_ended_at experiment_duration_seconds  \\\n",
       "0 2025-10-01 23:56:36.294169+00:00                  114.877647   \n",
       "1 2025-10-02 10:40:13.332620+00:00                  446.549921   \n",
       "2 2025-10-02 10:47:41.814046+00:00                  445.896804   \n",
       "3 2025-10-03 08:12:29.190901+00:00                  353.443725   \n",
       "4 2025-10-02 10:36:10.979403+00:00                  331.320460   \n",
       "\n",
       "  experiment_cost_usd experiment_n_calls experiment_step_count  \\\n",
       "0                 NaN                NaN                     2   \n",
       "1            0.920712               29.0                    29   \n",
       "2            0.552558               20.0                    20   \n",
       "3            0.020955               29.0                    29   \n",
       "4            0.042373               23.0                    23   \n",
       "\n",
       "  evaluation_started_at evaluation_ended_at  evaluation_duration_seconds  \\\n",
       "0                   NaN                 NaN                          NaN   \n",
       "1                   NaN                 NaN                          NaN   \n",
       "2                   NaN                 NaN                          NaN   \n",
       "3                   NaN                 NaN                          NaN   \n",
       "4                   NaN                 NaN                          NaN   \n",
       "\n",
       "   evaluation_step_count  evaluation_total_duration_seconds  \n",
       "0                    NaN                                NaN  \n",
       "1                    NaN                                NaN  \n",
       "2                    NaN                                NaN  \n",
       "3                    NaN                                NaN  \n",
       "4                    NaN                                NaN  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "def _get_agent_display(agent_config: AgentConfig) -> str:\n",
    "    \"\"\"Return a human-friendly label for the agent configuration.\"\"\"\n",
    "\n",
    "    if isinstance(agent_config, MiniSweAgentConfig):\n",
    "        return agent_config.model_name\n",
    "    return agent_config.display_name\n",
    "\n",
    "\n",
    "def _extract_experiment_common(\n",
    "    experiment: CompletedExperiment | FailedExperiment,\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"Collect fields shared by completed and failed experiments.\"\"\"\n",
    "\n",
    "    artifacts = experiment.artifacts\n",
    "    execution_trace = artifacts.execution_trace if artifacts else None\n",
    "    step_count = len(execution_trace.steps) if execution_trace else None\n",
    "\n",
    "    return {\n",
    "        \"experiment_id\": experiment.id,\n",
    "        \"experiment_started_at\": experiment.started_at,\n",
    "        \"experiment_ended_at\": experiment.ended_at,\n",
    "        \"experiment_duration_seconds\": experiment.duration.total_seconds(),\n",
    "        \"experiment_cost_usd\": artifacts.cost_usd if artifacts else None,\n",
    "        \"experiment_n_calls\": artifacts.n_calls if artifacts else None,\n",
    "        \"experiment_step_count\": step_count,\n",
    "    }\n",
    "\n",
    "\n",
    "records: list[dict[str, Any]] = []\n",
    "\n",
    "for task, result in data.items():\n",
    "    agent_cfg = task.agent_config\n",
    "    base: dict[str, Any] = {\n",
    "        \"instance_id\": task.instance_id,\n",
    "        \"agent_key\": agent_cfg.key,\n",
    "        \"agent_display_name\": _get_agent_display(agent_cfg),\n",
    "        \"agent_scaffold\": agent_cfg.scaffold,\n",
    "    }\n",
    "\n",
    "    if isinstance(result, FailedExperiment):\n",
    "        record = {\n",
    "            **base,\n",
    "            \"experiment_status\": \"failed\",\n",
    "            \"failure_category\": categorize_failure(result),\n",
    "            \"submission_id\": None,\n",
    "            \"empty_diff\": None,\n",
    "            \"evaluation_status\": None,\n",
    "            \"evaluation_id\": None,\n",
    "            \"evaluation_started_at\": None,\n",
    "            \"evaluation_ended_at\": None,\n",
    "            \"evaluation_duration_seconds\": None,\n",
    "            \"evaluation_step_count\": None,\n",
    "            \"evaluation_total_duration_seconds\": None,\n",
    "            \"patch_application_success\": None,  # TODO: shoudln't we set it to False ?\n",
    "            \"install_success\": None,  # TODO: shoudln't we set it to False ?\n",
    "            \"build_success\": None,  # TODO: shoudln't we set it to False ?\n",
    "            \"target_version_achieved\": None,  # TODO: shoudln't we set it to False ?\n",
    "        }\n",
    "        record.update(_extract_experiment_common(result))\n",
    "        records.append(record)\n",
    "        continue\n",
    "\n",
    "    experiment, evaluation = result\n",
    "    metrics = evaluation.result.metrics\n",
    "\n",
    "    record = {\n",
    "        **base,\n",
    "        \"experiment_status\": \"completed\",\n",
    "        \"failure_category\": None,\n",
    "        \"submission_id\": experiment.submission.submission_id,\n",
    "        \"empty_diff\": experiment.submission.model_patch.strip() == \"\",\n",
    "        \"evaluation_status\": \"completed\",\n",
    "        \"evaluation_id\": evaluation.id,\n",
    "        \"patch_application_success\": metrics.patch_application_success,\n",
    "        \"install_success\": metrics.install_success,\n",
    "        \"build_success\": metrics.build_success,\n",
    "        \"target_version_achieved\": metrics.target_version_achieved,\n",
    "    }\n",
    "    record.update(_extract_experiment_common(experiment))\n",
    "    records.append(record)\n",
    "\n",
    "analysis_df = (\n",
    "    pd.DataFrame.from_records(records)\n",
    "    .sort_values([\"instance_id\", \"agent_key\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "analysis_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost (USD): 21.6799\n"
     ]
    }
   ],
   "source": [
    "total_cost = analysis_df[\"experiment_cost_usd\"].dropna().sum()\n",
    "print(f\"Total cost (USD): {total_cost:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
