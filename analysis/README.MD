# Experiment Log Analysis

## Purpose

This directory is for the manual analysis of experiment logs generated by the BenchMAC framework.

The raw metrics (e.g., `BuildSuccess: True/False`) tell us *what* happened, but they don't tell us *why*. The goal of this analysis is to dive into the detailed logs to understand the behavior of the Systems Under Test (SUTs) and to verify the integrity of our evaluation harness.

The insights gathered here will form the core of the "Results" and "Discussion" sections of the research paper.

---

## The Two Core Questions of Our Analysis

Every log file should be examined with two main objectives in mind:

1.  **How did the AI agent perform its task?** We want to understand its strategy, its struggles, and the specific reasons for its failures.
2.  **Was the evaluation fair and reliable?** We need to identify any issues with our benchmark environment or harness that could have unfairly penalized (or helped) the agent.

---
### 1. Analyzing the AI Agent's Performance

Here, we are focused on understanding the SUT's capabilities and limitations by observing its actions. We aim to characterize its behavior across three key areas:

*   **Strategy and Planning:** What was the agent's overall approach? Did it follow a logical sequence of actions, or was its behavior chaotic and reactive?
*   **Reasoning and Knowledge:** Did the agent demonstrate an understanding of the technical domain and its tools? Or did it make fundamental errors related to the framework, language, or command-line usage?
*   **Problem-Solving and Resilience:** How did the agent react when faced with errors or unexpected outcomes? Did it get stuck in loops, give up prematurely, or effectively debug the problem?

---

### 2. Verifying the Harness and Environment

Here, we are ensuring the scientific validity of our benchmark. We need to confirm that any observed failures are the agent's fault, not a flaw in our experimental setup. We are looking for issues related to:

*   **Validity of the Task:** Did the experiment start from a correct and functional state (the "Green Baseline")? Any issues here would mean the task itself was flawed from the beginning.
*   **Stability of the Environment:** Was the sandbox environment consistent and free of external disruptions? We need to rule out problems like network failures, container crashes, or other non-deterministic behavior.
*   **Correctness of the Evaluation:** Did our harness accurately observe, record, and score the agent's actions? We are looking for any sign that the final metrics do not reflect what actually happened during the run.

The guiding principle is to answer: **Is this failure the agent's fault, or is it our fault?** Any issue that is not the agent's responsibility must be documented as a potential threat to the validity of our results.