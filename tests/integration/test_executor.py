from datetime import UTC, datetime

import pytest
from docker.errors import DockerException
from loguru import logger

from bench_mac.config import settings
from bench_mac.docker.manager import DockerManager
from bench_mac.executor import _is_peer_dep_error, execute_submission
from bench_mac.metrics import calculate_metrics
from bench_mac.models import (
    BenchmarkInstance,
    CommandOutput,
    ExecutionTrace,
    Submission,
)
from bench_mac.utils import load_instances

# A deliberately malformed patch that is guaranteed to fail application.
# It tries to remove a line that is unlikely to exist in that exact form.
BAD_PATCH = """
diff --git a/src/app/app.component.ts b/src/app/app.component.ts
index 1234567..abcdefg 100644
--- a/src/app/app.component.ts
+++ b/src/app/app.component.ts
@@ -1,5 +1,5 @@
-This line does not exist and will cause the patch to fail.
+This is the new line that will never be applied.
"""


@pytest.fixture(scope="module")
def docker_manager() -> DockerManager:
    """Provides a DockerManager instance, skipping tests if Docker is unavailable."""
    try:
        return DockerManager()
    except DockerException:
        pytest.skip("Docker daemon is not running. Skipping integration tests.")


@pytest.fixture(scope="module")
def test_instance() -> BenchmarkInstance:
    """Provides a real, known-good benchmark instance for testing."""
    # This instance is simple and its repository is small, making it ideal for testing.

    # Read the instances.jsonl file
    instances_file = settings.instances_file
    target_id = "gothinkster__angular-realworld-example-app_v11_to_v12"

    instances_map = load_instances(instances_file, strict=True)

    if target_id not in instances_map:
        raise ValueError(
            f"Instance with ID '{target_id}' not found in {instances_file}"
        )

    return instances_map[target_id]


@pytest.fixture(scope="module")
def silver_submission(test_instance: BenchmarkInstance) -> Submission:
    """
    Provides a known-good ("silver") submission for the test_instance.

    Raises
    ------
    FileNotFoundError
        If the required patch file does not exist, indicating a test setup error.
    """
    from bench_mac.config import settings

    patch_file = settings.silver_patches_dir / f"{test_instance.instance_id}.patch"

    if not patch_file.exists():
        error_message = (
            f"\n\nRequired silver patch not found at: {patch_file}\n\n"
            "This is a test setup error. To fix it:\n\n"
            "1. Ensure the patch has been generated by running:\n"
            "   uv run python scripts/generate_silvers.py\n\n"
            "2. Verify that the instance_id '{test_instance.instance_id}' used in this"
            " test has a corresponding entry in the SILVER_SOLUTIONS map within the"
            " 'scripts/generate_silvers.py' script."
        )
        raise FileNotFoundError(error_message)

    patch_content = patch_file.read_text()
    return Submission(instance_id=test_instance.instance_id, model_patch=patch_content)


def _find_step(trace: ExecutionTrace, command: str) -> CommandOutput | None:
    for step in trace.steps:
        if command in step.command:
            return step
    return None


@pytest.mark.integration
class TestExecuteSubmission:
    def test_successful_patch_application(
        self,
        docker_manager: DockerManager,
        test_instance: BenchmarkInstance,
        silver_submission: Submission,
    ) -> None:
        """
        Verify that a known-good patch is evaluated as successful.
        """
        # --- ACT ---
        trace = execute_submission(
            test_instance,
            silver_submission,
            docker_manager,
            logger=logger,
        )
        metrics = calculate_metrics(trace, test_instance)

        # --- ASSERT ---
        assert trace is not None
        assert len(trace.steps) > 0

        # The primary metric for this test
        assert metrics.patch_application_success is True

        # Check that the execution trace shows successful patch application
        patch_apply_step = _find_step(trace, "git apply -p0")

        assert patch_apply_step is not None
        assert patch_apply_step.success
        assert "error" not in patch_apply_step.stderr.lower()
        assert "fail" not in patch_apply_step.stderr.lower()

        # Check that the execution trace shows successful install
        # install_step = _find_step(trace, test_instance.commands.install)

        # assert install_step is not None
        # assert install_step.success

        # # Check that the execution trace shows successful build
        # build_step = _find_step(trace, test_instance.commands.build)

        # assert build_step is not None
        # assert build_step.success

        # TODO: check these steps from the metricsReport

    def test_failed_patch_application(
        self,
        docker_manager: DockerManager,
        test_instance: BenchmarkInstance,
    ) -> None:
        """
        Verify that a deliberately bad patch is evaluated as a failure.
        """
        # --- ARRANGE ---
        bad_submission = Submission(
            instance_id=test_instance.instance_id, model_patch=BAD_PATCH
        )

        # --- ACT ---
        trace = execute_submission(
            test_instance,
            bad_submission,
            docker_manager,
            logger=logger,
        )
        metrics = calculate_metrics(trace, test_instance)

        # --- ASSERT ---
        assert trace is not None
        assert len(trace.steps) > 0

        # The primary metric for this test
        assert metrics.patch_application_success is False

        # Check that the execution trace shows failed patch application
        patch_check_step = None
        for step in trace.steps:
            if "git apply --check" in step.command:
                patch_check_step = step
                break

        assert patch_check_step is not None
        assert not patch_check_step.success
        assert (
            "error" in patch_check_step.stderr.lower()
            or "patch does not apply" in patch_check_step.stderr.lower()
            or patch_check_step.exit_code != 0
        )


PEER_DEP_ERROR_STDERR = """npm ERR! code ERESOLVE
npm ERR! ERESOLVE unable to resolve dependency tree
npm ERR!
npm ERR! Found: @angular-devkit/build-angular@0.1102.5
npm ERR! node_modules/@angular-devkit/build-angular
npm ERR!   dev @angular-devkit/build-angular@"~0.1102.9" from the root project
npm ERR!
npm ERR! Could not resolve dependency:
npm ERR! dev @angular-devkit/build-angular@"~0.1102.9" from the root project
npm ERR!
npm ERR! Conflicting peer dependency: @angular/localize@11.2.10
npm ERR! node_modules/@angular/localize
npm ERR!   peerOptional @angular/localize@"^11.0.0 || ^11.2.0-next" from @angular-devkit/build-angular@0.1102.9
npm ERR!   node_modules/@angular-devkit/build-angular
npm ERR!     dev @angular-devkit/build-angular@"~0.1102.9" from the root project
npm ERR!
npm ERR! Fix the upstream dependency conflict, or retry
npm ERR! this command with --force, or --legacy-peer-deps
npm ERR! to accept an incorrect (and potentially broken) dependency resolution.
"""  # noqa: E501


@pytest.mark.unit
class TestIsPeerDepError:
    """Unit tests for the _is_peer_dep_error helper function."""

    def test_returns_true_for_standard_peer_dep_error(self) -> None:
        """Verify the function correctly identifies a standard npm ERESOLVE error."""
        failed_output = CommandOutput(
            command="npm ci",
            exit_code=1,
            stderr=PEER_DEP_ERROR_STDERR,
            start_time=datetime.now(UTC),
            end_time=datetime.now(UTC),
        )
        assert _is_peer_dep_error(failed_output) is True

    def test_is_case_insensitive(self) -> None:
        """Verify the check is case-insensitive as intended."""
        stderr = "ERESOLVE failed due to a CONFLICTING PEER DEPENDENCY."
        failed_output = CommandOutput(
            command="npm ci",
            exit_code=1,
            stderr=stderr,
            start_time=datetime.now(UTC),
            end_time=datetime.now(UTC),
        )
        assert _is_peer_dep_error(failed_output) is True

    def test_returns_false_if_command_was_successful(self) -> None:
        """A successful command should never be a peer dep error,
        even if stderr has the keywords."""
        stderr_with_keywords = (
            "Log: ERESOLVE check for conflicting peer dependency passed."
        )
        successful_output = CommandOutput(
            command="npm ci",
            exit_code=0,
            stderr=stderr_with_keywords,
            start_time=datetime.now(UTC),
            end_time=datetime.now(UTC),
        )
        assert _is_peer_dep_error(successful_output) is False

    @pytest.mark.parametrize(
        "other_error_message",
        [
            "npm ERR! code E404\nnpm ERR! 404 Not Found - GET https://registry.npmjs.org/...",
            "npm ERR! command failed",
            "Error: ENOENT: no such file or directory, open 'package.json'",
            "This is just some random error text.",
            "",
        ],
    )
    def test_returns_false_for_other_npm_or_generic_errors(
        self, other_error_message: str
    ) -> None:
        """Verify that other types of errors are not misidentified."""
        failed_output = CommandOutput(
            command="npm ci",
            exit_code=1,
            stderr=other_error_message,
            start_time=datetime.now(UTC),
            end_time=datetime.now(UTC),
        )
        assert _is_peer_dep_error(failed_output) is False
