import pytest
from docker.errors import DockerException
from loguru import logger

from bench_mac.docker.manager import DockerManager
from bench_mac.evaluator import evaluate_submission
from bench_mac.models import BenchmarkInstance, Submission

# A deliberately malformed patch that is guaranteed to fail application.
# It tries to remove a line that is unlikely to exist in that exact form.
BAD_PATCH = """
diff --git a/src/app/app.component.ts b/src/app/app.component.ts
index 1234567..abcdefg 100644
--- a/src/app/app.component.ts
+++ b/src/app/app.component.ts
@@ -1,5 +1,5 @@
-This line does not exist and will cause the patch to fail.
+This is the new line that will never be applied.
"""


@pytest.fixture(scope="module")
def docker_manager() -> DockerManager:
    """Provides a DockerManager instance, skipping tests if Docker is unavailable."""
    try:
        return DockerManager()
    except DockerException:
        pytest.skip("Docker daemon is not running. Skipping integration tests.")


@pytest.fixture(scope="module")
def test_instance() -> BenchmarkInstance:
    """Provides a real, known-good benchmark instance for testing."""
    # This instance is simple and its repository is small, making it ideal for testing.
    return BenchmarkInstance(
        instance_id="angular2-hn_v18_to_v19",
        repo="SuperMuel/angular2-hn",
        base_commit="e5a358f",  # Corresponds to v18
        source_angular_version="18",
        target_angular_version="19",
        target_node_version="20.11.0",
    )


@pytest.fixture(scope="module")
def silver_submission(test_instance: BenchmarkInstance) -> Submission:
    """
    Provides a known-good ("silver") submission for the test_instance.

    Raises
    ------
    FileNotFoundError
        If the required patch file does not exist, indicating a test setup error.
    """
    from bench_mac.config import settings

    patch_file = settings.silver_patches_dir / f"{test_instance.instance_id}.patch"

    if not patch_file.exists():
        error_message = (
            f"\n\nRequired silver patch not found at: {patch_file}\n\n"
            "This is a test setup error. To fix it:\n\n"
            "1. Ensure the patch has been generated by running:\n"
            "   uv run python scripts/generate_silvers.py\n\n"
            "2. Verify that the instance_id '{test_instance.instance_id}' used in this"
            " test has a corresponding entry in the SILVER_SOLUTIONS map within the"
            " 'scripts/generate_silvers.py' script."
        )
        raise FileNotFoundError(error_message)

    patch_content = patch_file.read_text()
    return Submission(instance_id=test_instance.instance_id, model_patch=patch_content)


@pytest.mark.integration
class TestEvaluateSubmission:
    def test_successful_patch_application(
        self,
        docker_manager: DockerManager,
        test_instance: BenchmarkInstance,
        silver_submission: Submission,
    ) -> None:
        """
        Verify that a known-good patch is evaluated as successful.
        """
        # --- ACT ---
        result = evaluate_submission(
            test_instance,
            silver_submission,
            docker_manager,
            logger=logger,
        )

        # --- ASSERT ---
        assert result is not None
        assert result.instance_id == test_instance.instance_id

        # The primary metric for this test
        assert result.metrics.patch_application_success is True

        # Check that the logs reflect success
        patch_log = result.logs.get("patch_apply", "")
        assert "error" not in patch_log.lower()
        assert "fail" not in patch_log.lower()

    def test_failed_patch_application(
        self,
        docker_manager: DockerManager,
        test_instance: BenchmarkInstance,
    ) -> None:
        """
        Verify that a deliberately bad patch is evaluated as a failure.
        """
        # --- ARRANGE ---
        bad_submission = Submission(
            instance_id=test_instance.instance_id, model_patch=BAD_PATCH
        )

        # --- ACT ---
        result = evaluate_submission(
            test_instance,
            bad_submission,
            docker_manager,
            logger=logger,
        )

        # --- ASSERT ---
        assert result is not None
        assert result.instance_id == test_instance.instance_id

        # The primary metric for this test
        assert result.metrics.patch_application_success is False

        # Check that the logs reflect the failure
        patch_log = result.logs.get("patch_apply", "")
        assert (
            "error" in patch_log.lower() or "patch does not apply" in patch_log.lower()
        )
