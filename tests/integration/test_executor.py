import pytest
from docker.errors import DockerException
from loguru import logger

from bench_mac.config import settings
from bench_mac.docker.manager import DockerManager
from bench_mac.evaluation import calculate_metrics
from bench_mac.executor import execute_submission
from bench_mac.models import (
    BenchmarkInstance,
    Submission,
)
from bench_mac.utils import load_instances

# A deliberately malformed patch that is guaranteed to fail application.
# It tries to remove a line that is unlikely to exist in that exact form.
BAD_PATCH = """
diff --git a/src/app/app.component.ts b/src/app/app.component.ts
index 1234567..abcdefg 100644
--- a/src/app/app.component.ts
+++ b/src/app/app.component.ts
@@ -1,5 +1,5 @@
-This line does not exist and will cause the patch to fail.
+This is the new line that will never be applied.
"""


@pytest.fixture(scope="module")
def docker_manager() -> DockerManager:
    """Provides a DockerManager instance, skipping tests if Docker is unavailable."""
    try:
        return DockerManager()
    except DockerException:
        pytest.skip("Docker daemon is not running. Skipping integration tests.")


@pytest.fixture(scope="module")
def test_instance() -> BenchmarkInstance:
    """Provides a real, known-good benchmark instance for testing."""
    # This instance is simple and its repository is small, making it ideal for testing.

    # Read the instances.jsonl file
    instances_file = settings.instances_file
    target_id = "gothinkster__angular-realworld-example-app_v11_to_v12"

    instances_map = load_instances(instances_file, strict=True)

    if target_id not in instances_map:
        raise ValueError(
            f"Instance with ID '{target_id}' not found in {instances_file}"
        )

    return instances_map[target_id]


@pytest.fixture(scope="module")
def silver_submission(test_instance: BenchmarkInstance) -> Submission:
    """
    Provides a known-good ("silver") submission for the test_instance.

    Raises
    ------
    FileNotFoundError
        If the required patch file does not exist, indicating a test setup error.
    """
    from bench_mac.config import settings

    patch_file = settings.silver_patches_dir / f"{test_instance.instance_id}.patch"

    if not patch_file.exists():
        error_message = (
            f"\n\nRequired silver patch not found at: {patch_file}\n\n"
            "This is a test setup error. To fix it:\n\n"
            "1. Ensure the patch has been generated by running:\n"
            "   uv run python scripts/generate_silvers.py\n\n"
            "2. Verify that the instance_id '{test_instance.instance_id}' used in this"
            " test has a corresponding entry in the SILVER_SOLUTIONS map within the"
            " 'scripts/generate_silvers.py' script."
        )
        raise FileNotFoundError(error_message)

    patch_content = patch_file.read_text()
    return Submission(instance_id=test_instance.instance_id, model_patch=patch_content)


@pytest.mark.integration
class TestExecuteSubmission:
    def test_successful_patch_application(
        self,
        docker_manager: DockerManager,
        test_instance: BenchmarkInstance,
        silver_submission: Submission,
    ) -> None:
        """
        Verify that a known-good patch is evaluated as successful.
        """
        # --- ACT ---
        trace = execute_submission(
            test_instance,
            silver_submission,
            docker_manager,
            logger=logger,
        )
        metrics = calculate_metrics(trace, test_instance)

        # --- ASSERT ---
        assert trace is not None
        assert len(trace.steps) > 0

        # The primary metric for this test
        assert metrics.patch_application_success is True

    def test_failed_patch_application(
        self,
        docker_manager: DockerManager,
        test_instance: BenchmarkInstance,
    ) -> None:
        """
        Verify that a deliberately bad patch is evaluated as a failure.
        """
        # --- ARRANGE ---
        bad_submission = Submission(
            instance_id=test_instance.instance_id, model_patch=BAD_PATCH
        )

        # --- ACT ---
        trace = execute_submission(
            test_instance,
            bad_submission,
            docker_manager,
            logger=logger,
        )
        metrics = calculate_metrics(trace, test_instance)

        # --- ASSERT ---
        assert trace is not None
        assert len(trace.steps) > 0

        # The primary metric for this test
        assert metrics.patch_application_success is False

        # Check that the execution trace shows failed patch application
        patch_check_step = None
        for step in trace.steps:
            if "git apply --check" in step.command:
                patch_check_step = step
                break

        assert patch_check_step is not None
        assert not patch_check_step.success
        assert (
            "error" in patch_check_step.stderr.lower()
            or "patch does not apply" in patch_check_step.stderr.lower()
            or patch_check_step.exit_code != 0
        )
